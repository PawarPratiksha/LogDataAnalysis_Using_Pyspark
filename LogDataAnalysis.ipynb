{"cells":[{"cell_type":"code","source":["#1\nimport sys\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import concat, col, lit\nfrom operator import add  \n\n\nspark = SparkSession.builder.appName('LogDataAnalysis').getOrCreate()\nInput_Data = [(\"2012-10-01\",\"02:13:48\",1061394,\"2.15.1\",\"i686\",\"linux-gnu\",\"Amelia\",\"1.6.3\",\"AU\",1),\n(\"2012-10-01\",\"02:37:34\",868687,\"2.15.0\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",3),\n(\"2012-10-01\",\"04:06:10\",1023,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"US\",4),\n(\"2012-10-01\",\"08:17:26\",2094435,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2),\n(\"2012-10-01\",\"08:29:01\",868687,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",2),\n(\"2012-10-01\",\"08:28:54\",2094449,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2)]\n\nInput_cols = [\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"]\nrdd = spark.sparkContext.parallelize(Input_Data)\n#rdd = rdd.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[6] + y[6], x[8] + y[8]))\n#rdd.collect()\n\n\nrdd1 = rdd.map(lambda x: (x[8], 1)).reduceByKey(lambda a,b:a +b)\n\nrdd1.collect()\nrdd2 = rdd.map(lambda x: (x[6], 1)).reduceByKey(lambda a,b:a +b)\nrdd3 = rdd2.union(rdd1).mapValues(lambda x:(x+1)).reduceByKey(lambda x,y : x+y)\nrdd3.collect()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2637e67e-b39c-4ed2-9589-a948601748c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[61]: [('NA', 2), ('RCurl', 3), ('AU', 2), ('Amelia', 2), ('US', 6), ('mosaic', 3)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[61]: [('NA', 2), ('RCurl', 3), ('AU', 2), ('Amelia', 2), ('US', 6), ('mosaic', 3)]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#3\nimport sys\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import concat, col, lit\n\nspark = SparkSession.builder.appName('LogDataAnalysis').getOrCreate()\nInput_Data = [(\"2012-10-01\",\"02:13:48\",1061394,\"2.15.1\",\"i686\",\"linux-gnu\",\"Amelia\",\"1.6.3\",\"AU\",1),\n(\"2012-10-01\",\"02:37:34\",868687,\"2.15.0\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",3),\n(\"2012-10-01\",\"04:06:10\",1023,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"US\",4),\n(\"2012-10-01\",\"08:17:26\",2094435,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2),\n(\"2012-10-01\",\"08:29:01\",868687,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",2),\n(\"2012-10-01\",\"08:28:54\",2094449,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2)]\n\nInput_cols = [\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"]\nrdd = spark.sparkContext.parallelize(Input_Data)\nrdd.collect()\n\ndf = spark.createDataFrame(Input_Data,Input_cols)\n\n#df.show(truncate=False)\n\n\ndf.filter((df.version=='NA') & (df.package=='NA')).show()\n\ndf = df.withColumn('logdate_logtime',f.concat(f.col('logdate'),f.lit('_'), f.col('logtime')))\n\n\ndf.sort(df.logdate_logtime.desc()).collect()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eaf9113-d8bc-4a2b-80de-0b18597d0dcc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------+----+---------+------+----+-------+-------+-------+-----+\n|   logdate| logtime|size|r_version|r_arch|r_os|package|version|country|ip_id|\n+----------+--------+----+---------+------+----+-------+-------+-------+-----+\n|2012-10-01|04:06:10|1023|       NA|    NA|  NA|     NA|     NA|     US|    4|\n+----------+--------+----+---------+------+----+-------+-------+-------+-----+\n\nOut[2]: [Row(logdate='2012-10-01', logtime='08:29:01', size=868687, r_version='2.15.1', r_arch='x86_64', r_os='linux-gnu', package='RCurl', version='1.95-0', country='US', ip_id=2, logdate_logtime='2012-10-01_08:29:01'),\n Row(logdate='2012-10-01', logtime='08:28:54', size=2094449, r_version='2.15.1', r_arch='x86_64', r_os='linux-gnu', package='mosaic', version='0.6-2', country='US', ip_id=2, logdate_logtime='2012-10-01_08:28:54'),\n Row(logdate='2012-10-01', logtime='08:17:26', size=2094435, r_version='2.15.1', r_arch='x86_64', r_os='linux-gnu', package='mosaic', version='0.6-2', country='US', ip_id=2, logdate_logtime='2012-10-01_08:17:26'),\n Row(logdate='2012-10-01', logtime='04:06:10', size=1023, r_version='NA', r_arch='NA', r_os='NA', package='NA', version='NA', country='US', ip_id=4, logdate_logtime='2012-10-01_04:06:10'),\n Row(logdate='2012-10-01', logtime='02:37:34', size=868687, r_version='2.15.0', r_arch='x86_64', r_os='linux-gnu', package='RCurl', version='1.95-0', country='US', ip_id=3, logdate_logtime='2012-10-01_02:37:34'),\n Row(logdate='2012-10-01', logtime='02:13:48', size=1061394, r_version='2.15.1', r_arch='i686', r_os='linux-gnu', package='Amelia', version='1.6.3', country='AU', ip_id=1, logdate_logtime='2012-10-01_02:13:48')]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------+----+---------+------+----+-------+-------+-------+-----+\n|   logdate| logtime|size|r_version|r_arch|r_os|package|version|country|ip_id|\n+----------+--------+----+---------+------+----+-------+-------+-------+-----+\n|2012-10-01|04:06:10|1023|       NA|    NA|  NA|     NA|     NA|     US|    4|\n+----------+--------+----+---------+------+----+-------+-------+-------+-----+\n\nOut[2]: [Row(logdate='2012-10-01', logtime='08:29:01', size=868687, r_version='2.15.1', r_arch='x86_64', r_os='linux-gnu', package='RCurl', version='1.95-0', country='US', ip_id=2, logdate_logtime='2012-10-01_08:29:01'),\n Row(logdate='2012-10-01', logtime='08:28:54', size=2094449, r_version='2.15.1', r_arch='x86_64', r_os='linux-gnu', package='mosaic', version='0.6-2', country='US', ip_id=2, logdate_logtime='2012-10-01_08:28:54'),\n Row(logdate='2012-10-01', logtime='08:17:26', size=2094435, r_version='2.15.1', r_arch='x86_64', r_os='linux-gnu', package='mosaic', version='0.6-2', country='US', ip_id=2, logdate_logtime='2012-10-01_08:17:26'),\n Row(logdate='2012-10-01', logtime='04:06:10', size=1023, r_version='NA', r_arch='NA', r_os='NA', package='NA', version='NA', country='US', ip_id=4, logdate_logtime='2012-10-01_04:06:10'),\n Row(logdate='2012-10-01', logtime='02:37:34', size=868687, r_version='2.15.0', r_arch='x86_64', r_os='linux-gnu', package='RCurl', version='1.95-0', country='US', ip_id=3, logdate_logtime='2012-10-01_02:37:34'),\n Row(logdate='2012-10-01', logtime='02:13:48', size=1061394, r_version='2.15.1', r_arch='i686', r_os='linux-gnu', package='Amelia', version='1.6.3', country='AU', ip_id=1, logdate_logtime='2012-10-01_02:13:48')]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#4\nimport sys\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import concat, col, lit\nfrom pyspark.sql.functions import when\n\nspark = SparkSession.builder.appName('LogDataAnalysis').getOrCreate()\nInput_Data = [(\"2012-10-01\",\"02:13:48\",1061394,\"2.15.1\",\"i686\",\"linux-gnu\",\"Amelia\",\"1.6.3\",\"AU\",1),\n(\"2012-10-01\",\"02:37:34\",868687,\"2.15.0\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",3),\n(\"2012-10-01\",\"04:06:10\",1023,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"US\",4),\n(\"2012-10-01\",\"08:17:26\",2094435,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2),\n(\"2012-10-01\",\"08:29:01\",868687,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",2),\n(\"2012-10-01\",\"08:28:54\",2094449,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2)]\n\nInput_cols = [\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"]\nrdd = spark.sparkContext.parallelize(Input_Data)\nrdd.collect()\n\ndf = spark.createDataFrame(Input_Data,Input_cols)\n\ndf.select(\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\",when(df['size'] <= 1000000,\"smalll\").otherwise(\"large\").alias(\"download_type\")).show() "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1ccd95bb-db1a-46b7-8870-08054e0591ab"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+--------+-------+---------+------+---------+-------+-------+-------+-----+-------------+\n|   logdate| logtime|   size|r_version|r_arch|     r_os|package|version|country|ip_id|download_type|\n+----------+--------+-------+---------+------+---------+-------+-------+-------+-----+-------------+\n|2012-10-01|02:13:48|1061394|   2.15.1|  i686|linux-gnu| Amelia|  1.6.3|     AU|    1|        large|\n|2012-10-01|02:37:34| 868687|   2.15.0|x86_64|linux-gnu|  RCurl| 1.95-0|     US|    3|       smalll|\n|2012-10-01|04:06:10|   1023|       NA|    NA|       NA|     NA|     NA|     US|    4|       smalll|\n|2012-10-01|08:17:26|2094435|   2.15.1|x86_64|linux-gnu| mosaic|  0.6-2|     US|    2|        large|\n|2012-10-01|08:29:01| 868687|   2.15.1|x86_64|linux-gnu|  RCurl| 1.95-0|     US|    2|       smalll|\n|2012-10-01|08:28:54|2094449|   2.15.1|x86_64|linux-gnu| mosaic|  0.6-2|     US|    2|        large|\n+----------+--------+-------+---------+------+---------+-------+-------+-------+-----+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+--------+-------+---------+------+---------+-------+-------+-------+-----+-------------+\n|   logdate| logtime|   size|r_version|r_arch|     r_os|package|version|country|ip_id|download_type|\n+----------+--------+-------+---------+------+---------+-------+-------+-------+-----+-------------+\n|2012-10-01|02:13:48|1061394|   2.15.1|  i686|linux-gnu| Amelia|  1.6.3|     AU|    1|        large|\n|2012-10-01|02:37:34| 868687|   2.15.0|x86_64|linux-gnu|  RCurl| 1.95-0|     US|    3|       smalll|\n|2012-10-01|04:06:10|   1023|       NA|    NA|       NA|     NA|     NA|     US|    4|       smalll|\n|2012-10-01|08:17:26|2094435|   2.15.1|x86_64|linux-gnu| mosaic|  0.6-2|     US|    2|        large|\n|2012-10-01|08:29:01| 868687|   2.15.1|x86_64|linux-gnu|  RCurl| 1.95-0|     US|    2|       smalll|\n|2012-10-01|08:28:54|2094449|   2.15.1|x86_64|linux-gnu| mosaic|  0.6-2|     US|    2|        large|\n+----------+--------+-------+---------+------+---------+-------+-------+-------+-----+-------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#5\nimport sys\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import concat, col, lit\nfrom pyspark.sql.functions import when\n\nspark = SparkSession.builder.appName('LogDataAnalysis').getOrCreate()\nInput_Data = [(\"2012-10-01\",\"02:13:48\",1061394,\"2.15.1\",\"i686\",\"linux-gnu\",\"Amelia\",\"1.6.3\",\"AU\",1),\n(\"2012-10-01\",\"02:37:34\",868687,\"2.15.0\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",3),\n(\"2012-10-01\",\"04:06:10\",1023,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"US\",4),\n(\"2012-10-01\",\"08:17:26\",2094435,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2),\n(\"2012-10-01\",\"08:29:01\",868687,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",2),\n(\"2012-10-01\",\"08:28:54\",2094449,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2)]\n\nInput_cols = [\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"]\nrdd = spark.sparkContext.parallelize(Input_Data)\nrdd.collect()\n\ndf = spark.createDataFrame(Input_Data,Input_cols)\ndf.groupBy('package').count().select('package', f.col('count').alias('package_count')).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f01cb99-3b43-472e-b334-d4be437f2a52"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+-------+-------------+\n|package|package_count|\n+-------+-------------+\n| Amelia|            1|\n|  RCurl|            2|\n|     NA|            1|\n| mosaic|            2|\n+-------+-------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+-------+-------------+\n|package|package_count|\n+-------+-------------+\n| Amelia|            1|\n|  RCurl|            2|\n|     NA|            1|\n| mosaic|            2|\n+-------+-------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#6\nimport sys\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import concat, col, lit\nfrom pyspark.sql.functions import when\n\nspark = SparkSession.builder.appName('LogDataAnalysis').getOrCreate()\nInput_Data = [(\"2012-10-01\",\"02:13:48\",1061394,\"2.15.1\",\"i686\",\"linux-gnu\",\"Amelia\",\"1.6.3\",\"AU\",1),\n(\"2012-10-01\",\"02:37:34\",868687,\"2.15.0\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",3),\n(\"2012-10-01\",\"04:06:10\",1023,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"US\",4),\n(\"2012-10-01\",\"08:17:26\",2094435,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2),\n(\"2012-10-01\",\"08:29:01\",868687,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",2),\n(\"2012-10-01\",\"08:28:54\",2094449,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2)]\n\nInput_cols = [\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"]\nrdd = spark.sparkContext.parallelize(Input_Data)\nrdd.collect()\n\ndf = spark.createDataFrame(Input_Data,Input_cols)\n\ndf = df.select(\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\",when(df['size'] <= 1000000,\"smalll\").otherwise(\"large\").alias(\"download_type\"))\n#df = df.select([f.sum(df.size).alias('total_size')]).show()\ndf.groupBy(\"logdate\",\"download_type\").agg(f.sum(\"size\").alias(\"total_size\"),f.round(f.avg(\"size\")).alias(\"average_size\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b5fc4d7-be36-4619-b24e-920ce3517f54"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"+----------+-------------+----------+------------+\n|   logdate|download_type|total_size|average_size|\n+----------+-------------+----------+------------+\n|2012-10-01|        large|   5250278|   1750093.0|\n|2012-10-01|       smalll|   1738397|    579466.0|\n+----------+-------------+----------+------------+\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["+----------+-------------+----------+------------+\n|   logdate|download_type|total_size|average_size|\n+----------+-------------+----------+------------+\n|2012-10-01|        large|   5250278|   1750093.0|\n|2012-10-01|       smalll|   1738397|    579466.0|\n+----------+-------------+----------+------------+\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\nresult = rdd.groupBy(lambda x: x % 2).collect()\nsorted([(x, sorted(y)) for (x, y) in result])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbfbb510-6eb8-4d62-bb26-b29df676653e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[16]: [(0, [2, 8]), (1, [1, 1, 3, 5])]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[16]: [(0, [2, 8]), (1, [1, 1, 3, 5])]"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["#2\nimport sys\nfrom pyspark import SparkContext\nimport pyspark.sql.functions as f\nfrom pyspark.sql.types import IntegerType\nfrom pyspark.sql import SparkSession\nfrom pyspark import SparkContext\nfrom pyspark.sql import SQLContext\nfrom pyspark.sql.functions import concat, col, lit\n\n\n\nspark = SparkSession.builder.appName('LogDataAnalysis').getOrCreate()\nInput_Data = [(\"2012-10-01\",\"02:13:48\",1061394,\"2.15.1\",\"i686\",\"linux-gnu\",\"Amelia\",\"1.6.3\",\"AU\",1),\n(\"2012-10-01\",\"02:37:34\",868687,\"2.15.0\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",3),\n(\"2012-10-01\",\"04:06:10\",1023,\"NA\",\"NA\",\"NA\",\"NA\",\"NA\",\"US\",4),\n(\"2012-10-01\",\"08:17:26\",2094435,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2),\n(\"2012-10-01\",\"08:29:01\",868687,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"RCurl\",\"1.95-0\",\"US\",2),\n(\"2012-10-01\",\"08:28:54\",2094449,\"2.15.1\",\"x86_64\",\"linux-gnu\",\"mosaic\",\"0.6-2\",\"US\",2)]\n\nInput_cols = [\"logdate\",\"logtime\",\"size\",\"r_version\",\"r_arch\",\"r_os\",\"package\",\"version\",\"country\",\"ip_id\"]\nrdd = spark.sparkContext.parallelize(Input_Data)\n\naccum = spark.sparkContext.accumulator(0)\nrdd2 = rdd.filter(lambda x: x[2] >= 1000000)\nrdd2.foreach(lambda x: accum.add(x[2]))\naccum.value"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1dfa6396-2f58-452b-b8e3-f4d76e47ec9d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[87]: 5250278","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[87]: 5250278"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b627679e-6570-4e0f-8443-8a899f1f124c"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"LogDataAnalysis","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":2260106238879962}},"nbformat":4,"nbformat_minor":0}
